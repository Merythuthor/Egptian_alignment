{"name": "baseline MLM",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp3_baseline_new/\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 0.0 --translation_weight 0.0 --pos_weight 0.0  --consistency_lambda 0.0 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --normalization_mode NONE --fusion_mode none\n"}
{"name": "MLM+TLM",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp2_MLM_TLM/\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 1.0 --translation_weight 0.0 --pos_weight 0.0 --consistency_lambda 0.0 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --normalization_mode NONE --fusion_mode none"}
{"name": "MLM+Trans",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp2_MLM_Translation/\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 0.0 --translation_weight 1.0 --pos_weight 0.0 --with_translation --consistency_lambda 0.0 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --normalization_mode NONE --fusion_mode none"}
{"name": "MLM+TLM+Trans",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp2_MLM_TLM_Translation/\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 1.0 --translation_weight 1.0 --pos_weight 0.0 --with_translation --consistency_lambda 0.0 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --normalization_mode NONE --fusion_mode none"}
{"name": "MLM+TLM+Trans+POS",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp2_balanced_new/\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 1.0 --translation_weight 1.0 --pos_weight 0.5 --with_translation --consistency_lambda 0.0 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --normalization_mode NONE --fusion_mode none"}
{"name": "MLM Fusion Latin",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp_MLM_fusion_alpha_latin_new_2\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 0.0 --translation_weight 0.0 --pos_weight 0.0 --normalization_mode LATIN --consistency_lambda 0.0 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --fusion_mode alpha"}
{"name": "MLM KL Latin",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp3_MLM_KL_Latin_latest\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 0.0 --translation_weight 0.0 --pos_weight 0.0 --normalization_mode LATIN --consistency_lambda 0.5 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --fusion_mode none"}
{"name": "MLM Fusion IPA",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp_MLM_fusion_alpha_IPA/\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 0.0 --translation_weight 0.0 --pos_weight 0.0 --normalization_mode IPA --consistency_lambda 0.0 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --fusion_mode alpha"}
{"name": "MLM KL IPA",
 "cmd":"python -m training.train_main --model_size bert_all --lang hieroglyphic,demotic,bohairic,sahidic --multi --tokenizer_type shared_bpe --output_dir \"checkpoints/bert_all_exp_MLM_KL_ipa/test\" --epochs 10 --batch_size 16 --eval_batch_size 32 --lr 5e-5 --lr_scheduler_type cosine --warmup_steps 500 --weight_decay 0.01 --dropout 0.1 --attention_dropout 0.1 --mlm_weight 1.0 --tlm_weight 0.0 --translation_weight 0.0 --pos_weight 0.0 --normalization_mode IPA --consistency_lambda 0.5 --eval_strategy epoch --use_wandb --fp16 --gradient_accumulation_steps 2 --fusion_mode none"}